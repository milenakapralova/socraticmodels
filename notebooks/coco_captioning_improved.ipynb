{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ff268",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Caption Generation (improved) | DL2 Project, May 2023\n",
    "---\n",
    "\n",
    "This notebook downloads the images from the validation split of the [MS COCO Dataset (2017 version)](https://cocodataset.org/#download) and the corresponding ground-truth captions and generates captions based on the Socratic model pipeline outlined below. In this notebook, we propose a new method to obtain image captions via the Socratic method:\n",
    "* <span style=\"color:#006400\">**Improved prompting**</span>: an improved baseline model where the template prompt filled by CLIP is processed before passing to [FLAN-T5-xl](https://huggingface.co/docs/transformers/model_doc/flan-t5).\n",
    "\n",
    "In other words, this is an improved pipeline that has for goal to generate similar or improved captions using open-source and free models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e331e",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "If you haven't done so already, please activate the corresponding environment by running in the terminal: `conda env create -f environment.yml`. Then type `conda activate socratic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1503d1d",
   "metadata": {},
   "source": [
    "### Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11989003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'image_captioning'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Local imports\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimage_captioning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ClipManager, ImageManager, VocabManager, LMManager, COCOManager\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimage_captioning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LmPromptGenerator \u001B[38;5;28;01mas\u001B[39;00m pg\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_device\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'image_captioning'"
     ]
    }
   ],
   "source": [
    "# Package loading\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Local imports\n",
    "from scripts.image_captioning import ClipManager, ImageManager, VocabManager, LmManager, CocoManager\n",
    "from scripts.image_captioning import LmPromptGenerator as pg\n",
    "from scripts.image_captioning import CacheManager as cm\n",
    "from scripts.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b652f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d341a53",
   "metadata": {},
   "source": [
    "### Set seeds for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace seed\n",
    "set_seed(42)\n",
    "\n",
    "# Set seed for 100 random images of the MS COCO validation split\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada75e",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the MS COCO images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898c7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_folder = 'imgs/val2017/'\n",
    "annotation_file = 'annotations/annotations/captions_val2017.json'\n",
    "\n",
    "coco_manager = CocoManager()\n",
    "coco_manager.download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c54da",
   "metadata": {},
   "source": [
    "## Step 2: Generating the captions via the Socratic pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aab5ad",
   "metadata": {},
   "source": [
    "### Set the device and instantiate managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bcac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.0s!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1238bdb27c184ecabc4bc5ff6d295e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the device to use\n",
    "device = get_device()\n",
    "\n",
    "# Instantiate the clip manager\n",
    "clip_manager = ClipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()\n",
    "\n",
    "# Instantiate the vocab manager\n",
    "vocab_manager = VocabManager()\n",
    "\n",
    "# Instantiate the Flan T5 manager\n",
    "flan_manager = LmManager(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d924",
   "metadata": {},
   "source": [
    "### Compute place and object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372c21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the place features\n",
    "if not os.path.exists('cache/place_emb.npy'):\n",
    "    # Calculate the place features\n",
    "    place_emb = clip_manager.get_text_emb([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "    np.save('cache/place_emb.npy', place_emb)\n",
    "else:\n",
    "    place_emb = np.load('cache/place_emb.npy')\n",
    "\n",
    "# Calculate the object features\n",
    "if not os.path.exists('cache/object_emb.npy'):\n",
    "    # Calculate the object features\n",
    "    object_emb = clip_manager.get_text_emb([f'Photo of a {o}.' for o in vocab_manager.object_list])\n",
    "    np.save('cache/object_emb.npy', object_emb)\n",
    "else:\n",
    "    object_emb = np.load('cache/object_emb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59985dbb",
   "metadata": {},
   "source": [
    "### Load images and compute image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d44faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'improved'\n",
    "\n",
    "img_dic = {}\n",
    "img_feat_dic = {}\n",
    "img_paths = {}\n",
    "# if not os.path.exists(f'{approach}_outputs.csv'):\n",
    "    # N = len(os.listdir(imgs_folder))\n",
    "N = 100\n",
    "random_numbers = random.sample(range(len(os.listdir(imgs_folder))), N)\n",
    "\n",
    "# for ix, file_name in enumerate(os.listdir(imgs_folder)[:N]):\n",
    "for ix, file_name in enumerate(os.listdir(imgs_folder)):\n",
    "     # Consider only image files that are part of the random sample\n",
    "    if file_name.endswith(\".jpg\") and ix in random_numbers:  \n",
    "        # Getting image id\n",
    "        file_name_strip = file_name.strip('.jpg')\n",
    "        match = re.search('^0+', file_name_strip)\n",
    "        sequence = match.group(0)\n",
    "        image_id = int(file_name_strip[len(sequence):])\n",
    "\n",
    "        img_path = os.path.join(imgs_folder, file_name)\n",
    "        img = image_manager.load_image(img_path)\n",
    "        img_emb = clip_manager.get_img_emb(img)\n",
    "        img_emb = img_emb.flatten()\n",
    "        img_paths[image_id] = file_name\n",
    "\n",
    "        img_dic[image_id] = img\n",
    "        img_feat_dic[image_id] = img_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13299759",
   "metadata": {},
   "source": [
    "### Zero-shot VLM (CLIP)\n",
    "We zero-shot prompt CLIP to produce various inferences of an iage, such as image type or the number of people in an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6632d",
   "metadata": {},
   "source": [
    "#### Classify image type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be894f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_types = ['photo', 'cartoon', 'sketch', 'painting']\n",
    "img_types_emb = clip_manager.get_text_emb([f'This is a {t}.' for t in img_types])\n",
    "\n",
    "# Create a dictionary to store the image types\n",
    "img_type_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_img_types, img_type_scores = clip_manager.get_nn_text(img_types, img_types_emb, img_feat)\n",
    "    img_type_dic[img_name] = sorted_img_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3867d",
   "metadata": {},
   "source": [
    "#### Classify number of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_texts = [\n",
    "    'are no people', 'is one person', 'are two people', 'are three people', 'are several people', 'are many people'\n",
    "]\n",
    "ppl_emb = clip_manager.get_text_emb([f'There {p} in this photo.' for p in ppl_texts])\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "num_people_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_ppl_texts, ppl_scores = clip_manager.get_nn_text(ppl_texts, ppl_emb, img_feat)\n",
    "    num_people_dic[img_name] = sorted_ppl_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4549f",
   "metadata": {},
   "source": [
    "#### Classify image place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_topk = 3\n",
    "\n",
    "# Create a dictionary to store the number of people\n",
    "location_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_places, places_scores = clip_manager.get_nn_text(vocab_manager.place_list, place_emb, img_feat)\n",
    "    location_dic[img_name] = sorted_places[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf872a1",
   "metadata": {},
   "source": [
    "#### Classify image object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_topk = 10\n",
    "\n",
    "# Create a dictionary to store the similarity of each object with the images\n",
    "object_score_map = {}\n",
    "sorted_obj_dic = {}\n",
    "for img_name, img_feat in img_feat_dic.items():\n",
    "    sorted_obj_texts, obj_scores = clip_manager.get_nn_text(vocab_manager.object_list, object_emb, img_feat)\n",
    "    object_score_map[img_name] = dict(zip(sorted_obj_texts, obj_scores))\n",
    "    sorted_obj_dic[img_name] = sorted_obj_texts\n",
    "    object_list = ''\n",
    "    for i in range(obj_topk):\n",
    "        object_list += f'{sorted_obj_texts[i]}, '\n",
    "    object_list = object_list[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67070d36",
   "metadata": {},
   "source": [
    "#### Finding both relevant and different objects using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc25e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps the objects to the cosine sim.\n",
    "object_embeddings = dict(zip(vocab_manager.object_list, object_emb))\n",
    "\n",
    "# Create a dictionary to store the terms to include\n",
    "terms_to_include = {}\n",
    "\n",
    "for img_name, sorted_obj_texts in sorted_obj_dic.items():\n",
    "\n",
    "    # Create a list that contains the objects ordered by cosine sim.\n",
    "    embeddings_sorted = [object_embeddings[w] for w in sorted_obj_texts]\n",
    "\n",
    "    # Create a list to store the best matches\n",
    "    best_matches = [sorted_obj_texts[0]]\n",
    "\n",
    "    # Create an array to store the embeddings of the best matches\n",
    "    unique_embeddings = embeddings_sorted[0].reshape(-1, 1)\n",
    "\n",
    "    # Loop through the 100 best objects by cosine similarity\n",
    "    for i in range(1, 100):\n",
    "        # Obtain the maximum cosine similarity when comparing object i to the embeddings of the current best matches\n",
    "        max_cos_sim = (unique_embeddings.T @ embeddings_sorted[i]).max()\n",
    "        # If object i is different enough to the current best matches, add it to the best matches\n",
    "        if max_cos_sim < 0.7:\n",
    "            unique_embeddings = np.concatenate([unique_embeddings, embeddings_sorted[i].reshape(-1, 1)], 1)\n",
    "            best_matches.append(sorted_obj_texts[i])\n",
    "\n",
    "    # Looping through the best matches, consider each terms separately by splitting the commas and spaces.\n",
    "    data_list = []\n",
    "    for terms in best_matches:\n",
    "        for term_split in terms.split(', '):\n",
    "            score = clip_manager.get_image_caption_score(term_split, img_feat_dic[img_name])\n",
    "            data_list.append({\n",
    "                'term': term_split, 'score': score, 'context': terms\n",
    "            })\n",
    "            term_split_split = term_split.split(' ')\n",
    "            if len(term_split_split) > 1:\n",
    "                for term_split2 in term_split_split:\n",
    "                    score = clip_manager.get_image_caption_score(term_split2, img_feat_dic[img_name])\n",
    "                    data_list.append({\n",
    "                        'term': term_split2, 'score': score, 'context': terms\n",
    "                    })\n",
    "\n",
    "    # Create a dataframe with the terms and scores and only keep the top term per context.\n",
    "    term_df = pd.DataFrame(data_list).sort_values('score', ascending=False).drop_duplicates('context').reset_index(drop=True)\n",
    "\n",
    "    # Prepare loop to find if additional terms can improve cosine similarity\n",
    "    best_terms_sorted = term_df['term'].tolist()\n",
    "    best_term = best_terms_sorted[0]\n",
    "    terms_to_check = list(set(best_terms_sorted[1:]))\n",
    "    best_cos_sim = term_df['score'].iloc[0]\n",
    "    terms_to_include[img_name] = [best_term]\n",
    "\n",
    "    # Perform a loop to find if additional terms can improve the cosine similarity\n",
    "    n_iteration = 5\n",
    "    for iteration in range(n_iteration):\n",
    "        data_list = []\n",
    "        for term_to_test in terms_to_check:\n",
    "            new_term = f\"{best_term} {term_to_test}\"\n",
    "            score = clip_manager.get_image_caption_score(new_term, img_feat_dic[img_name])\n",
    "            data_list.append({\n",
    "                'term': new_term, 'candidate': term_to_test, 'score': score\n",
    "            })\n",
    "        combined_df = pd.DataFrame(data_list).sort_values('score', ascending=False)\n",
    "        if combined_df['score'].iloc[0] > best_cos_sim + 0.01:\n",
    "            best_cos_sim = combined_df['score'].iloc[0]\n",
    "            terms_to_include[img_name].append(combined_df['candidate'].iloc[0])\n",
    "            terms_to_check = combined_df['candidate'].tolist()[1:]\n",
    "            best_term += f\" {combined_df['candidate'].iloc[0]}\"\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e564fd3",
   "metadata": {},
   "source": [
    "#### Generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_captions = 50\n",
    "\n",
    "# Set LM params\n",
    "model_params = {'temperature': 0.9, 'max_length': 40, 'do_sample': True}\n",
    "\n",
    "# Create dictionaries to store the outputs\n",
    "prompt_dic = {}\n",
    "sorted_caption_map = {}\n",
    "caption_score_map = {}\n",
    "\n",
    "for img_name in img_dic:\n",
    "    # Create the prompt for the language model\n",
    "    prompt_dic[img_name] = pg.create_improved_lm_prompt(\n",
    "        img_type_dic[img_name], num_people_dic[img_name], terms_to_include[img_name]\n",
    "    )\n",
    "\n",
    "    # Generate the caption using the language model\n",
    "    caption_texts = flan_manager.generate_response(num_captions * [prompt_dic[img_name]], model_params)\n",
    "\n",
    "    # Zero-shot VLM: rank captions.\n",
    "    caption_emb = clip_manager.get_text_emb(caption_texts)\n",
    "    sorted_captions, caption_scores = clip_manager.get_nn_text(caption_texts, caption_emb, img_feat_dic[img_name])\n",
    "    sorted_caption_map[img_name] = sorted_captions\n",
    "    caption_score_map[img_name] = dict(zip(sorted_captions, caption_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260f29b",
   "metadata": {},
   "source": [
    "### Save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for img_name in img_dic:\n",
    "    generated_caption = sorted_caption_map[img_name][0]\n",
    "    data_list.append({\n",
    "        'image_name': img_name,\n",
    "        'image_path': img_paths[img_name],\n",
    "        'generated_caption': generated_caption,\n",
    "        'cosine_similarity': caption_score_map[img_name][generated_caption]\n",
    "    })\n",
    "pd.DataFrame(data_list).to_csv(f'{approach}_outputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
