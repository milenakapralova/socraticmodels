{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHdM8FYSMaBx"
      },
      "source": [
        "# Baseline evaluation of FLAN-T5 on the MS COCO image-caption pairs dataset\n",
        "\n",
        "---\n",
        "The following metrics will be used, based on [this](https://github.com/salaniz/pycocoevalcap) repository:\n",
        "\n",
        "* **BLEU-4**: BLEU (Bilingual Evaluation Understudy) is a metric that measures the similarity between the generated captions and the ground truth captions based on n-gram matching. The BLEU-4 score measures the precision of the generated captions up to four-grams compared to the ground truth captions.\n",
        "\n",
        "* **METEOR**: METEOR (Metric for Evaluation of Translation with Explicit ORdering) is another metric that measures the similarity between the generated captions and the ground truth captions. It also takes into account word order and synonymy by using a set of reference summaries to compute a harmonic mean of precision and recall.\n",
        "\n",
        "* **CIDEr**: CIDEr (Consensus-based Image Description Evaluation) is a metric that measures the consensus between the generated captions and the ground truth captions. It computes the similarity between the generated captions and the reference captions based on their TF-IDF weights, which helps capture important words in the captions.\n",
        "\n",
        "* **SPICE**: SPICE (Semantic Propositional Image Caption Evaluation) is a metric that measures the semantic similarity between the generated captions and the ground truth captions. It analyzes the similarity between the semantic propositions present in the generated captions and those in the reference captions, taking into account the structure and meaning of the propositions.\n",
        "\n",
        "* **ROUGE-L**: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric that measures the similarity between the generated captions and the ground truth captions based on overlapping sequences of words. ROUGE-L measures the longest common subsequence (LCS) between the generated captions and the reference captions, taking into account sentence-level structure and word order.\n",
        "\n",
        "---"
      ],
      "id": "MHdM8FYSMaBx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLNsr9JuOiLd"
      },
      "source": [
        "We first install the required evaluation scripts and libraries:"
      ],
      "id": "wLNsr9JuOiLd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks2nKFpeDN-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6feca533-a91d-4a5d-860a-82cfa658f162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.22.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap.git\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-req-build-6orfjmlj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-req-build-6orfjmlj\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap==1.2) (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.39.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycocotools\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap.git"
      ],
      "id": "ks2nKFpeDN-J"
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "# from pycocoevalcap.eval import COCOEvalCap\n",
        "import pycocoevalcap\n",
        "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "\n",
        "import urllib.request\n",
        "import json\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "h03MtfnjP5oK"
      },
      "id": "h03MtfnjP5oK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "aDUzvl4SPMEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6724dee6-65d8-4f18-f8e9-84fc82471210"
      },
      "id": "aDUzvl4SPMEV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2kYDP9VOYi0"
      },
      "source": [
        "If you are running this notebook for the first time, please download the dataset and annotations from the COCO website, mount the Google Drive and save the downloaded files. To do the above, please uncomment the following cell and then comment out again:"
      ],
      "id": "k2kYDP9VOYi0"
    },
    {
      "cell_type": "code",
      "source": [
        "## Download the annotations file from the COCO website\n",
        "# annotations_url = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n",
        "# urllib.request.urlretrieve(annotations_url, 'annotations.zip')\n",
        "# !unzip annotations.zip\n",
        "\n",
        "## Copy the file to your Google Drive\n",
        "# import shutil\n",
        "# shutil.copy('annotations/captions_val2014.json', '/content/gdrive/MyDrive/')"
      ],
      "metadata": {
        "id": "zkysRfLlQFlK"
      },
      "id": "zkysRfLlQFlK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ground truth (GT) annotations"
      ],
      "metadata": {
        "id": "yUL5dyQ09Lsy"
      },
      "id": "yUL5dyQ09Lsy"
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_file = '/content/gdrive/MyDrive/captions_val2014.json'\n",
        "coco = COCO(annotations_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywQ6iNYLRTMM",
        "outputId": "0585f3ac-482c-495b-fb03-fd730cab7e9e"
      },
      "id": "ywQ6iNYLRTMM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.46s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated (GEN) annotations"
      ],
      "metadata": {
        "id": "rsQZQUiA9RZH"
      },
      "id": "rsQZQUiA9RZH"
    },
    {
      "cell_type": "code",
      "source": [
        "# change this file to FLAN-T5 generated caption text file\n",
        "captions_file = '/content/gdrive/MyDrive/captions_val2014.json'\n",
        "with open(captions_file, 'r') as f:\n",
        "    captions = json.load(f)"
      ],
      "metadata": {
        "id": "G_ZBfFmcQFpd"
      },
      "id": "G_ZBfFmcQFpd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load the ground truth captions and the generated captions into Python dictionaries. Each caption should be a string."
      ],
      "metadata": {
        "id": "BK4muwEKIhrZ"
      },
      "id": "BK4muwEKIhrZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with GT and GEN captions for each image\n",
        "# gt_captions = []\n",
        "# gen_captions = []\n",
        "gt_captions = {}\n",
        "gen_captions = {}"
      ],
      "metadata": {
        "id": "NR1AqUd2RBwg"
      },
      "id": "NR1AqUd2RBwg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image_id, caption in captions.items():\n",
        "for ix in range(len(captions['annotations'])):\n",
        "    \n",
        "    # remove this later\n",
        "    (_, image_id), (_, id), (_, caption) = captions['annotations'][ix].items()\n",
        "\n",
        "    # get the IDs of the annotations for the given image ID\n",
        "    ann_ids = coco.getAnnIds(imgIds=image_id)\n",
        "\n",
        "    # load the annotations for the given annotation IDs\n",
        "    anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "    # extract the reference captions from the annotations\n",
        "    references = [ann['caption'] for ann in anns]\n",
        "\n",
        "    # add the reference captions and generated caption to the lists\n",
        "    # gt_captions.append(references)\n",
        "    # gen_captions.append(caption)\n",
        "    gt_captions[image_id] = [{'caption': '\\n'.join(references)}]\n",
        "    # gen_captions[image_id] = caption\n",
        "    gen_captions[image_id] = [{'caption': caption}]"
      ],
      "metadata": {
        "id": "QRg4_aVVRByu"
      },
      "id": "QRg4_aVVRByu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(gt_captions)} ground truth (reference) captions. An example reference captions from the annotations are... \\n{gt_captions[167549]}\\n')\n",
        "\n",
        "print(f'There are {len(gen_captions)} ground truth (reference) captions. An example generated caption from the model is... \\n{gen_captions[167549]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHZJYs-UcYYK",
        "outputId": "9d28b44b-505d-4803-d20c-8aba48d7242d"
      },
      "id": "IHZJYs-UcYYK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 40504 ground truth (reference) captions. An example reference captions from the annotations are... \n",
            "[{'caption': 'A stuffed bear and a vase by a headstone.\\nA brown teddy bear holding a glass vase in front of a grave.\\nA stuffed animal is in she snow in front of a tombstone.\\nA gravestone with a vase and stuffed animal on it.\\nA stuffed animal and a vase by a gravestone.'}]\n",
            "\n",
            "There are 40504 ground truth (reference) captions. An example generated caption from the model is... \n",
            "[{'caption': 'A stuffed animal and a vase by a gravestone.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenize the captions using the PTBTokenizer provided by the COCO evaluation toolkit."
      ],
      "metadata": {
        "id": "8nPoG4uQKFEI"
      },
      "id": "8nPoG4uQKFEI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise tokenizer\n",
        "tokenizer = PTBTokenizer()\n",
        "\n",
        "gt_captions_tokens = tokenizer.tokenize(gt_captions)\n",
        "gen_captions_tokens = tokenizer.tokenize(gen_captions)"
      ],
      "metadata": {
        "id": "qPABZD1xOF-0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "332d3b93-89d6-4537-fddd-ac830eecfe14"
      },
      "id": "qPABZD1xOF-0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4cb6c9f8d08e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialise tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTBTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgt_captions_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgen_captions_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PTBTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize the evaluation metrics that you want to use (BLEU, ROUGE, CIDEr, and/or SPICE) and compute the evaluation scores for the generated captions with respect to the ground truth captions."
      ],
      "metadata": {
        "id": "DEgx_bCDUFmz"
      },
      "id": "DEgx_bCDUFmz"
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_eval = Bleu()\n",
        "meteor_eval = Meteor()\n",
        "cider_eval = Cider()\n",
        "spice_eval = Spice()\n",
        "rouge_eval = Rouge()"
      ],
      "metadata": {
        "id": "jpRzSKamUECJ"
      },
      "id": "jpRzSKamUECJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute BLEU score\n",
        "bleu_score, _ = bleu_eval.compute_score(gt_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Compute METEOR score\n",
        "meteor_score, _ = meteor_eval.compute_score(gt_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Compute CIDEr score\n",
        "cider_score, _ = cider_eval.compute_score(gt_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Compute SPICE score\n",
        "# spice_score, _ = spice_eval.compute_score(gt_captions, gen_captions)\n",
        "\n",
        "# Compute ROUGE score\n",
        "rouge_score, _ = rouge_eval.compute_score(gt_captions_tokens, gen_captions_tokens)"
      ],
      "metadata": {
        "id": "sUti34XLVt7S"
      },
      "id": "sUti34XLVt7S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We print the evaluation scores\n",
        "print(\"BLEU score: \", bleu_score)\n",
        "print(\"METEOR score: \", meteor_score)\n",
        "print(\"CIDEr score: \", cider_score)\n",
        "# print(\"SPICE score: \", spice_score)\n",
        "print(\"ROUGE score: \", rouge_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dPClvgAWNQr",
        "outputId": "f36aae82-0875-4ad4-d5ad-471e02587281"
      },
      "id": "5dPClvgAWNQr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score:  [0.018309205099127902, 0.01830920281982215, 0.018309200181732745, 0.018309197076799424]\n",
            "METEOR score:  0.10194725233647307\n",
            "CIDEr score:  4.8458567515162145e-08\n",
            "ROUGE score:  0.29645632888870255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To put these values in perspective, we can see what the best possible scores are on each of the above metric:"
      ],
      "metadata": {
        "id": "iwKc8o_36gk4"
      },
      "id": "iwKc8o_36gk4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Best possible BLEU score\n",
        "best_bleu_score, _ = bleu_eval.compute_score(gen_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Best possible METEOR score\n",
        "best_meteor_score, _ = meteor_eval.compute_score(gen_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Best possible CIDEr score\n",
        "best_cider_score, _ = cider_eval.compute_score(gen_captions_tokens, gen_captions_tokens)\n",
        "\n",
        "# Best possible SPICE score\n",
        "best_spice_score, _ = spice_eval.compute_score(gen_captions, gen_captions)\n",
        "\n",
        "# Best possible ROUGE score\n",
        "best_rouge_score, _ = rouge_eval.compute_score(gen_captions_tokens, gen_captions_tokens)"
      ],
      "metadata": {
        "id": "pyY5-YoNUEZ0"
      },
      "id": "pyY5-YoNUEZ0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We print the evaluation scores\n",
        "print(\"Best BLEU score: \", best_bleu_score)\n",
        "print(\"Best METEOR score: \", best_meteor_score)\n",
        "# print(\"SPICE score: \", spice_score)\n",
        "print(\"Best CIDEr score: \", best_cider_score)\n",
        "print(\"Best ROUGE score: \", best_rouge_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym9LIzRBlNAQ",
        "outputId": "00301273-bfc3-4e7b-d605-e73f8aaf9cd6"
      },
      "id": "ym9LIzRBlNAQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best BLEU score:  [0.9999999999999952, 0.9999999999999951, 0.999999999999995, 0.9999999999999948]\n",
            "Best METEOR score:  1.0\n",
            "Best CIDEr score:  10.0\n",
            "Best ROUGE score:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "* The annotations are not available for the test split, only for the train and validation splits. Should we use the validation split?\n",
        "* Three versions of the MS COCO dataset are available: 2014, 2015, 2017. Which one is best to use?\n",
        "* Should we evaluate the caption quality on other dataset?\n",
        "* Should FLAN-T5 be evaluated on the whole validation split or only on the random 100 samples (possibly sampled multiple times and averaged)?\n",
        "* How to evaluate performance on single generated caption against 5 ground truth captions?\n",
        "* Other possible metrics to consider other than those of the source paper:\n",
        "    1. Take GT and GEN caption embeddings from CLIP --> Take cosine similarity/dot product \n",
        "    * Motivation: Rule-based methods do not usually capture the semantics of the caption, only operate on the token level.\n",
        "    2. A [learning based](https://vision.cornell.edu/se3/wp-content/uploads/2018/03/1501.pdf) discriinative evaluation metric\n",
        "    * Motivation: Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence.\n",
        "* SPICE metric does not work at the moment."
      ],
      "metadata": {
        "id": "fFkMu8yx3Krt"
      },
      "id": "fFkMu8yx3Krt"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMjrq_ZPQF0q"
      },
      "id": "TMjrq_ZPQF0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWQ_VchMPhaJ"
      },
      "id": "rWQ_VchMPhaJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7SX-wMKPhcf"
      },
      "id": "R7SX-wMKPhcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLC405JgPhfG"
      },
      "id": "RLC405JgPhfG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRs9YbV1eH8M"
      },
      "id": "rRs9YbV1eH8M",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}