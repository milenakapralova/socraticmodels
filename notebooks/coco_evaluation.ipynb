{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ff268",
   "metadata": {},
   "source": [
    "# SocraticFlanT5 - Evaluation | DL2 Project, May 2023\n",
    "---\n",
    "\n",
    "This notebook evaluates the generated captions based on the MS COCO ground-truth captions. We will evaluate the folowing two approaches: \n",
    "1. <span style=\"color:#006400\">**Baseline**</span>: a Socratic model based on the work by [Zeng et al. (2022)](https://socraticmodels.github.io/) where GPT-3 is replaced by [FLAN-T5-xl](https://huggingface.co/docs/transformers/model_doc/flan-t5). \n",
    "\n",
    "2. <span style=\"color:#006400\">**Improved prompting**</span>: an improved baseline model where the template prompt filled by CLIP is processed before passing to FLAN-T5-xl.\n",
    "\n",
    "There are two approaches to this evaluation: rule-based and embedding-based.\n",
    "\n",
    "---\n",
    "For the **rule-based approach**, the following metrics will be used, based on [this](https://github.com/salaniz/pycocoevalcap) repository:\n",
    "\n",
    "* *BLEU-4*: BLEU (Bilingual Evaluation Understudy) is a metric that measures the similarity between the generated captions and the ground truth captions based on n-gram matching. The BLEU-4 score measures the precision of the generated captions up to four-grams compared to the ground truth captions.\n",
    "\n",
    "* *METEOR*: METEOR (Metric for Evaluation of Translation with Explicit ORdering) is another metric that measures the similarity between the generated captions and the ground truth captions. It also takes into account word order and synonymy by using a set of reference summaries to compute a harmonic mean of precision and recall.\n",
    "\n",
    "* *CIDEr*: CIDEr (Consensus-based Image Description Evaluation) is a metric that measures the consensus between the generated captions and the ground truth captions. It computes the similarity between the generated captions and the reference captions based on their TF-IDF weights, which helps capture important words in the captions.\n",
    "\n",
    "* *SPICE*: SPICE (Semantic Propositional Image Caption Evaluation) is a metric that measures the semantic similarity between the generated captions and the ground truth captions. It analyzes the similarity between the semantic propositions present in the generated captions and those in the reference captions, taking into account the structure and meaning of the propositions.\n",
    "\n",
    "* *ROUGE-L*: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric that measures the similarity between the generated captions and the ground truth captions based on overlapping sequences of words. ROUGE-L measures the longest common subsequence (LCS) between the generated captions and the reference captions, taking into account sentence-level structure and word order.\n",
    "\n",
    "---\n",
    "\n",
    "For the **embedding-based** approach (based on CLIP embeddings), we calculate the cosine similarities between each image embedding and embeddings of the ground truth captions and then we calculate the cosine similarities between each image embedding and embeddings of the captions generated with FLAN-T5-xl.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597939c",
   "metadata": {},
   "source": [
    "<span style=\"color:#8B0000\">**Important**</span>: we assume that you have the generated captions accessible from the current directory via `cache/baseline_outputs.csv` or `cache/improved_outputs.csv` or both. If that is not the case, please run (either or both of) the following notebooks:\n",
    "* `SocraticFlanT5 - Caption Generation_baseline.ipynb`\n",
    "* `SocraticFlanT5 - Caption Generation_improved.ipynb`\n",
    "\n",
    "Moreover, we assume you have pre-computed the image embeddings and have them stored and accessible from the current directory via `cache/embed_imgs.pickle`. If that is not the case, please run (either or both of) the following notebook:\n",
    "* `SocraticFlanT5 - Image Embedding Generation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e331e",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1503d1d",
   "metadata": {},
   "source": [
    "#### Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11989003",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "from scripts.image_captioning import ClipManager\n",
    "from scripts.coco_evaluation import SocraticEvalCap\n",
    "from scripts.utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1967d18",
   "metadata": {},
   "source": [
    "### Evaluate the generated captions against the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44bc3d",
   "metadata": {},
   "source": [
    "#### Load the ground truth annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0daf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = 'annotations/annotations/captions_val2017.json'\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    lines = json.load(f)['annotations']\n",
    "gts = {}\n",
    "for item in lines:\n",
    "    if item['image_id'] not in gts:\n",
    "        gts[item['image_id']] = []\n",
    "    gts[item['image_id']].append({'image_id': item['image_id'], 'caption': item['caption']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0ff84",
   "metadata": {},
   "source": [
    "#### Compute the embeddings for the gt captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b42d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cache/embed_capt_gt.pickle'):\n",
    "    # Set the device to use\n",
    "    device = get_device()\n",
    "\n",
    "    # Instantiate the clip manager\n",
    "    clip_manager = ClipManager(device)\n",
    "    \n",
    "    embed_capt_gt = {}\n",
    "    for img_id, list_of_capt_dict in gts.items():\n",
    "        list_of_captions = [capt_dict['caption'] for capt_dict in list_of_capt_dict]\n",
    "\n",
    "        # Dims of img_emb_gt: 5 x 768\n",
    "        img_emb_gt = clip_manager.get_text_emb(list_of_captions)\n",
    "\n",
    "        embed_capt_gt[img_id] = img_emb_gt\n",
    "\n",
    "    with open('cache/embed_capt_gt.pickle', 'wb') as handle:\n",
    "        pickle.dump(embed_capt_gt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30577e5e",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7530ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 125 tokens at 1826.59 tokens per second.\n",
      "PTBTokenizer tokenized 58 tokens at 869.81 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 46, 'reflen': 25, 'guess': [46, 44, 42, 40], 'correct': [6, 1, 0, 0]}\n",
      "ratio: 1.8399999999264\n",
      "Bleu_1: 0.130\n",
      "Bleu_2: 0.054\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.058\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.159\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.028\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.068 s\n",
      "SPICE: 0.030\n",
      "Bleu_1: 0.130\n",
      "Bleu_2: 0.054\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "METEOR: 0.058\n",
      "ROUGE_L: 0.159\n",
      "CIDEr: 0.028\n",
      "SPICE: 0.030\n",
      "gts: avg = 0.241, std = 0.027\n",
      "res: avg = 0.188, std = 0.005\n"
     ]
    }
   ],
   "source": [
    "approaches = ['baseline', 'improved']\n",
    "# approaches = ['baseline']\n",
    "\n",
    "\n",
    "eval_cap = {\n",
    "    'rulebased': {},\n",
    "    'cossim': {}\n",
    "}\n",
    "\n",
    "for approach in approaches:\n",
    "    # Load the generated captions\n",
    "    res_raw = pd.read_csv(f'{approach}_outputs.csv')\n",
    "\n",
    "    \n",
    "    evaluator = SocraticEvalCap(gts, res_raw)\n",
    "\n",
    "    # Rule-based metrics\n",
    "    evaluator.evaluate_rulebased()\n",
    "    eval_rulebased = {}\n",
    "\n",
    "    for metric, score in evaluator.eval.items():\n",
    "        print(f'{metric}: {score:.3f}')\n",
    "        eval_rulebased[metric] = round(score, 5)\n",
    "    eval_cap['rulebased'][approach] = eval_rulebased\n",
    "\n",
    "    # Embedding-based metric\n",
    "    evaluator.evaluate_cossim()\n",
    "    for source_caption, sim in evaluator.sims.items():\n",
    "        print(f'{source_caption}: avg = {sim[0]:.3f}, std = {sim[1]:.3f}')\n",
    "    eval_cap['cossim'][approach] = evaluator.sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305f506",
   "metadata": {},
   "source": [
    "### Save the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48481684",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_cap.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_cap, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ba34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
